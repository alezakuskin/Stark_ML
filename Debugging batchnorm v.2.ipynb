{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Creating classes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E5MrtNW8V1KJ"
      ],
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a250780adad47dea6eaf81dca2887b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_192aa0879e27494b8a40d1ce74312a12",
              "IPY_MODEL_b66be8e16e6d42948f42fbebec9ac70d",
              "IPY_MODEL_b68b99b9b1b74dbdaf81b76d0547fab8"
            ],
            "layout": "IPY_MODEL_af111a16c5694ef8b6589644ceeb58f1"
          }
        },
        "192aa0879e27494b8a40d1ce74312a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a53757e1f84f43569eda9cc2466fd618",
            "placeholder": "​",
            "style": "IPY_MODEL_82a71f80e91e4b5eb8fec621d1088e5c",
            "value": " 16%"
          }
        },
        "b66be8e16e6d42948f42fbebec9ac70d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ac92d5194b466584905a119dcbfba9",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_596ef9458afb4fc7a10e96644c988183",
            "value": 8
          }
        },
        "b68b99b9b1b74dbdaf81b76d0547fab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48b70c6c4c7420e96221c404dcdce74",
            "placeholder": "​",
            "style": "IPY_MODEL_c03bd7e16c7345d7bead9cd33438b3cd",
            "value": " 8/50 [1:32:23&lt;8:45:02, 750.07s/it]"
          }
        },
        "af111a16c5694ef8b6589644ceeb58f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a53757e1f84f43569eda9cc2466fd618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82a71f80e91e4b5eb8fec621d1088e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33ac92d5194b466584905a119dcbfba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "596ef9458afb4fc7a10e96644c988183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e48b70c6c4c7420e96221c404dcdce74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c03bd7e16c7345d7bead9cd33438b3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet\n",
        "!cd /usr/local/python3.7/dist-packages/pytorch_tabnet && patch </content/float64.patch\n",
        "\n",
        "\n",
        "import os\n",
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "\n",
        "!pip install optuna\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import optuna\n",
        "from google.colab import output\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from pytorch_tabnet.tab_model import  TabNetRegressor\n",
        "\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "u0T8S_xHsBRP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import pytorch_tabnet\n",
        "importlib.reload(pytorch_tabnet)"
      ],
      "metadata": {
        "id": "5pGu4oOV3Ekl",
        "outputId": "80b584d7-88cc-4daa-ffa5-20694a3e7120",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'pytorch_tabnet' (namespace)>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Objective(object):\n",
        "    def __init__(self, model_name, X, y, params):\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Save the trainings data\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.params = params\n",
        "\n",
        "        \n",
        "    def __call__(self, trial):\n",
        "        # Define hyperparameters to optimize\n",
        "        trial_params = self.model_name.define_trial_parameters(trial, self.params)\n",
        "        print(trial_params)\n",
        "        \n",
        "        score = 0\n",
        "        # Cross validate the chosen hyperparameters\n",
        "\n",
        "        kf = KFold(self.params['nfold'], shuffle = False)\n",
        "        for train, test in kf.split(self.X):\n",
        "            X_train, y_train = self.X.iloc[train, :], self.y.iloc[train]\n",
        "            X_val, y_val = self.X.iloc[test, :], self.y.iloc[test]\n",
        "            \n",
        "            model = self.model_name(trial_params)\n",
        "            model.fit(X_train, y_train, X_val, y_val)\n",
        "            score += mean_squared_error(y_val, model.predict(X_val),\n",
        "                                        squared = self.params['squared_metrics'])\n",
        "\n",
        "        score /= self.params['nfold']\n",
        "        \n",
        "        return score\n",
        "\n",
        "\n",
        "def main(X, y, model_name, params, n_trials = 100):\n",
        "    print(\"Start hyperparameter optimization\")\n",
        "    \n",
        "    Sampler = optuna.samplers.TPESampler(seed = 777)\n",
        "    study = optuna.create_study(sampler = Sampler)\n",
        "    study.optimize(Objective(model_name, X, y, params), n_trials, show_progress_bar = True, n_jobs = 1)\n",
        "    \n",
        "    print(\"Best parameters:\", study.best_trial.params)\n",
        "\n",
        "    return study"
      ],
      "metadata": {
        "id": "MuGwONKNukQ9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TabNet():\n",
        "\n",
        "    def __init__(self, params):\n",
        "        \n",
        "        self.model = TabNetRegressor(**params, verbose = False, device_name = 'cpu')\n",
        "        #if torch.cuda.is_available():\n",
        "        #    self.model.to('cuda')\n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        X = X.to_numpy()\n",
        "        y = y.to_numpy().reshape(-1, 1)\n",
        "        \n",
        "        if isinstance(X_val, pd.DataFrame):\n",
        "            X_val, y_val = X_val.to_numpy(), y_val.to_numpy().reshape(-1, 1)\n",
        "            \n",
        "        self.model.fit(X, y, eval_set = [(X_val, y_val)], eval_name = ['eval'], max_epochs = 500, patience = 20)\n",
        "        history = self.model.history\n",
        "        return history['loss']\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = X.to_numpy()\n",
        "\n",
        "        return self.model.predict(X)\n",
        "        \n",
        "    @classmethod\n",
        "    def define_trial_parameters(cls, trial, params):\n",
        "        params_tunable = {}\n",
        "        params_out = {}\n",
        "        for i, val in params.items():\n",
        "            if isinstance(val, list):\n",
        "                params_tunable[f'{i}'] = val\n",
        "            else:\n",
        "                params_out[f'{i}'] = val\n",
        "        \n",
        "        if 'n_d' in params_tunable:\n",
        "            params_out[f'n_d'] = trial.suggest_int('n_d', params['n_d'][0], params['n_d'][1], log = False)\n",
        "        if 'n_steps' in params_tunable:\n",
        "            params_out[f'n_steps'] = trial.suggest_int('n_steps', params['n_steps'][0], params['n_steps'][1], log = False)\n",
        "        if 'gamma' in params_tunable:\n",
        "            params_out[f'gamma'] = trial.suggest_float('gamma', params['gamma'][0], params['gamma'][1], log = False)\n",
        "        if 'cat_emb_dim' in params_tunable:\n",
        "            params_out[f'cat_emb_dim'] = trial.suggest_int('cat_emb_dim', params['cat_emb_dim'][0], params['cat_emb_dim'][1], log = False)\n",
        "        if 'n_independent' in params_tunable:\n",
        "            params_out[f'n_independent'] = trial.suggest_int('n_independent', params['n_independent'][0], params['n_independent'][1], log = False)\n",
        "        if 'n_shared' in params_tunable:\n",
        "            params_out[f'n_shared'] = trial.suggest_int('n_shared', params['n_shared'][0], params['n_shared'][1], log = False)\n",
        "        if 'momentum' in params_tunable:\n",
        "            params_out[f'momentum'] = trial.suggest_float('momentum', params['momentum'][0], params['momentum'][1], log = True)\n",
        "        if 'mask_type' in params_tunable:\n",
        "            params_out[f'mask_type'] = trial.suggest_categorical('mask_type', params['mask_type'])\n",
        "        \n",
        "        \n",
        "        if 'nfold' in params_out:\n",
        "            del params_out['nfold']\n",
        "        if 'squared_metrics' in params_out:\n",
        "            del params_out['squared_metrics']\n",
        "        \n",
        "        return params_out"
      ],
      "metadata": {
        "id": "GKqdTyCVtzwO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(7)\n",
        "X = np.random.randint(0, 11, size = (745, 50))\n",
        "y = np.random.rand(745) * 175"
      ],
      "metadata": {
        "id": "Nwpw5oRFsGup"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(X)\n",
        "y = pd.DataFrame(y)\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "AZml6V81u4M_",
        "outputId": "702b3fb6-3525-4cec-ec46-3cf3fcfd68f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "def module_has_nan(m):\n",
        "    ret = functools.reduce(\n",
        "        lambda ret, module: ret or functools.reduce(\n",
        "            lambda ret, parameter: (ret or parameter.isnan().any().cpu().numpy()).any(),\n",
        "            module.parameters(), False\n",
        "        ), m.modules(), False\n",
        "    )\n",
        "    print(ret)\n",
        "    return ret"
      ],
      "metadata": {
        "id": "BO1wI2Z0nEJd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "failed_params = {'n_d': 2, 'n_steps': 23, 'gamma': 1.3362582829842025, 'n_independent': 18, 'n_shared': 6, 'momentum': 0.0060940631018202435, 'mask_type': 'entmax'}\n",
        "\n",
        "score = 0\n",
        "# Cross validate the chosen hyperparameters\n",
        "\n",
        "kf = KFold(5, shuffle = False)\n",
        "for train, test in kf.split(X):\n",
        "    X_train, y_train = X.iloc[train, :], y.iloc[train]\n",
        "    X_val, y_val = X.iloc[test, :], y.iloc[test]\n",
        "    \n",
        "    model = TabNetRegressor(**failed_params, verbose = True, device_name = 'cuda')\n",
        "\n",
        "    X = X_train.to_numpy().astype(np.float64)\n",
        "    y = y_train.to_numpy().reshape(-1, 1).astype(np.float64)\n",
        "        \n",
        "    if isinstance(X_val, pd.DataFrame):\n",
        "        X_val, y_val = X_val.to_numpy(), y_val.to_numpy().reshape(-1, 1)\n",
        "\n",
        "    model.fit(X, y, eval_set = [(X_val, y_val)], eval_name = ['eval'], max_epochs = 500, patience = 20)\n",
        "    module_has_nan(model.network)\n",
        "    print()\n",
        "\n",
        "    score += mean_squared_error(y_val, model.predict(X_val),\n",
        "                                squared = False)\n",
        "\n",
        "score /= 5"
      ],
      "metadata": {
        "id": "t-eSl3yqmnAL",
        "outputId": "281c2809-33cc-4545-d4b5-b26b82479975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used : cuda\n",
            "epoch 0  | loss: 10974.82711| eval_mse: 2044538699.63964|  0:00:01s\n",
            "epoch 1  | loss: 11009.60241| eval_mse: 845740477.28573|  0:00:02s\n",
            "epoch 2  | loss: 10947.17714| eval_mse: 227494150.28563|  0:00:03s\n",
            "epoch 3  | loss: 11022.88373| eval_mse: 155486311.95783|  0:00:04s\n",
            "epoch 4  | loss: 10993.46461| eval_mse: 79800478.68356|  0:00:06s\n",
            "epoch 5  | loss: 10948.6001| eval_mse: 42369398.12749|  0:00:07s\n",
            "epoch 6  | loss: 10978.69974| eval_mse: 24321578.65051|  0:00:08s\n",
            "epoch 7  | loss: 11007.02591| eval_mse: 13017295.00968|  0:00:09s\n",
            "epoch 8  | loss: 10988.90641| eval_mse: 23292570.28343|  0:00:11s\n",
            "epoch 9  | loss: 10999.3067| eval_mse: 4963329.62921|  0:00:12s\n",
            "epoch 10 | loss: 11067.60201| eval_mse: 2609405.74334|  0:00:13s\n",
            "epoch 11 | loss: 10986.05107| eval_mse: 3169869.68824|  0:00:14s\n",
            "epoch 12 | loss: 11020.58315| eval_mse: 2220864.70622|  0:00:15s\n",
            "epoch 13 | loss: 10966.25843| eval_mse: 1314556.77798|  0:00:17s\n",
            "epoch 14 | loss: 10946.65865| eval_mse: 1167119.30034|  0:00:18s\n",
            "epoch 15 | loss: 10950.84462| eval_mse: 771874.06661|  0:00:19s\n",
            "epoch 16 | loss: 11052.01567| eval_mse: 1102966.77718|  0:00:20s\n",
            "epoch 17 | loss: 10990.91557| eval_mse: 597830.63889|  0:00:21s\n",
            "epoch 18 | loss: 11008.56812| eval_mse: 433047.31233|  0:00:23s\n",
            "epoch 19 | loss: 10914.57373| eval_mse: 305711.88174|  0:00:24s\n",
            "epoch 20 | loss: 10986.72774| eval_mse: 275780.6674|  0:00:25s\n",
            "epoch 21 | loss: 10919.08989| eval_mse: 314291.09328|  0:00:26s\n",
            "epoch 22 | loss: 10985.88489| eval_mse: 505685.88095|  0:00:27s\n",
            "epoch 23 | loss: 11034.18326| eval_mse: 416161.11724|  0:00:29s\n",
            "epoch 24 | loss: 10986.32966| eval_mse: 139691.55983|  0:00:30s\n",
            "epoch 25 | loss: 11027.60436| eval_mse: 167606.00092|  0:00:31s\n",
            "epoch 26 | loss: 11017.71468| eval_mse: 114199.61126|  0:00:32s\n",
            "epoch 27 | loss: 11003.36178| eval_mse: 126918.88006|  0:00:33s\n",
            "epoch 28 | loss: 11014.58372| eval_mse: 105458.47181|  0:00:34s\n",
            "epoch 29 | loss: 11074.47383| eval_mse: 71380.95961|  0:00:35s\n",
            "epoch 30 | loss: 11036.99663| eval_mse: 70988.23468|  0:00:37s\n",
            "epoch 31 | loss: 10999.10418| eval_mse: 75982.00412|  0:00:38s\n",
            "epoch 32 | loss: 11066.71046| eval_mse: 59592.73081|  0:00:39s\n",
            "epoch 33 | loss: 11067.4382| eval_mse: 61878.95782|  0:00:40s\n",
            "epoch 34 | loss: 11040.52929| eval_mse: 65931.60011|  0:00:41s\n",
            "epoch 35 | loss: 11079.76511| eval_mse: 62928.36775|  0:00:42s\n",
            "epoch 36 | loss: 11082.90097| eval_mse: 91111.29499|  0:00:43s\n",
            "epoch 37 | loss: 10985.89843| eval_mse: 76236.23942|  0:00:44s\n",
            "epoch 38 | loss: 11045.94805| eval_mse: 122081.58634|  0:00:45s\n",
            "epoch 39 | loss: 11002.74701| eval_mse: 91265.42579|  0:00:46s\n",
            "epoch 40 | loss: 11058.17919| eval_mse: 65201.61147|  0:00:47s\n",
            "epoch 41 | loss: 11039.18411| eval_mse: 88131.1801|  0:00:48s\n",
            "epoch 42 | loss: 11009.39423| eval_mse: 78903.66764|  0:00:49s\n",
            "epoch 43 | loss: 10992.30931| eval_mse: 82969.32019|  0:00:50s\n",
            "epoch 44 | loss: 11038.72812| eval_mse: 95986.97916|  0:00:51s\n",
            "epoch 45 | loss: 11014.7496| eval_mse: 61664.59537|  0:00:52s\n",
            "epoch 46 | loss: 10984.91167| eval_mse: 71870.49932|  0:00:54s\n",
            "epoch 47 | loss: 11096.14261| eval_mse: 62353.34726|  0:00:55s\n",
            "epoch 48 | loss: 11028.7472| eval_mse: 25451.58439|  0:00:56s\n",
            "epoch 49 | loss: 11062.08436| eval_mse: 19326.34334|  0:00:57s\n",
            "epoch 50 | loss: 11005.58651| eval_mse: 40996.58918|  0:00:58s\n",
            "epoch 51 | loss: 11037.51467| eval_mse: 30521.07758|  0:00:59s\n",
            "epoch 52 | loss: 10962.10638| eval_mse: 38148.44489|  0:01:00s\n",
            "epoch 53 | loss: 11011.22545| eval_mse: 27046.96711|  0:01:01s\n",
            "epoch 54 | loss: 11083.59154| eval_mse: 28003.34838|  0:01:02s\n",
            "epoch 55 | loss: 11089.96126| eval_mse: 24893.87719|  0:01:03s\n",
            "epoch 56 | loss: 11075.8294| eval_mse: 29236.91224|  0:01:04s\n",
            "epoch 57 | loss: 11071.26718| eval_mse: 24138.38284|  0:01:05s\n",
            "epoch 58 | loss: 11082.77551| eval_mse: 29632.75811|  0:01:06s\n",
            "epoch 59 | loss: 11083.33295| eval_mse: 21991.04674|  0:01:07s\n",
            "epoch 60 | loss: 11059.06227| eval_mse: 31942.62027|  0:01:08s\n",
            "epoch 61 | loss: 11004.31796| eval_mse: 30816.99508|  0:01:09s\n",
            "epoch 62 | loss: 11077.74584| eval_mse: 23567.71975|  0:01:10s\n",
            "epoch 63 | loss: 11069.48666| eval_mse: 25591.21658|  0:01:12s\n",
            "epoch 64 | loss: 11033.9548| eval_mse: 31882.88777|  0:01:13s\n",
            "epoch 65 | loss: 11018.01925| eval_mse: 28459.70568|  0:01:14s\n",
            "epoch 66 | loss: 11056.05987| eval_mse: 41415.61046|  0:01:15s\n",
            "epoch 67 | loss: 10980.98702| eval_mse: 28056.89063|  0:01:16s\n",
            "epoch 68 | loss: 11010.21635| eval_mse: 32356.04276|  0:01:17s\n",
            "epoch 69 | loss: 11023.69302| eval_mse: 26440.47817|  0:01:18s\n",
            "\n",
            "Early stopping occurred at epoch 69 with best_epoch = 49 and best_eval_mse = 19326.34334\n",
            "Best weights from best epoch are automatically used!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fb5ac78a3277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodule_has_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-50b5723c9b43>\u001b[0m in \u001b[0;36mmodule_has_nan\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         ), m.modules(), False\n\u001b[0m\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-50b5723c9b43>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(ret, module)\u001b[0m\n\u001b[1;32m      4\u001b[0m         lambda ret, module: ret or functools.reduce(\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         ), m.modules(), False\n\u001b[1;32m      8\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-7-50b5723c9b43>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(ret, parameter)\u001b[0m\n\u001b[1;32m      3\u001b[0m     ret = functools.reduce(\n\u001b[1;32m      4\u001b[0m         lambda ret, module: ret or functools.reduce(\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0;32mlambda\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         ), m.modules(), False\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module_has_nan(model.network)"
      ],
      "metadata": {
        "id": "3aCVJBJvMLrs",
        "outputId": "9e412b93-53f1-41a1-eab5-7028a701c43c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%debug"
      ],
      "metadata": {
        "id": "a2aOvQ9eoenp",
        "outputId": "ee54cbd8-a19e-4890-a903-d88ec8fcde29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m(2283)\u001b[0;36mbatch_norm\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   2281 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   2282 \u001b[0;31m    return torch.batch_norm(\n",
            "\u001b[0m\u001b[0;32m-> 2283 \u001b[0;31m        \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   2284 \u001b[0;31m    )\n",
            "\u001b[0m\u001b[0;32m   2285 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "tensor([[ 4.,  9.,  2.,  ..., 10.,  9.,  0.],\n",
            "        [ 7.,  6.,  6.,  ...,  7.,  1.,  4.],\n",
            "        [ 7.,  8.,  2.,  ...,  3.,  9.,  6.],\n",
            "        ...,\n",
            "        [ 5.,  4.,  6.,  ...,  3.,  1.,  8.],\n",
            "        [ 4.,  1.,  8.,  ...,  4.,  7.,  9.],\n",
            "        [ 8.,  2.,  6.,  ...,  8.,  6.,  6.]], dtype=torch.float32)\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       requires_grad=True)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "torch.float64\n",
            "torch.float64\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m(179)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    177 \u001b[0;31m            \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    178 \u001b[0;31m            \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 179 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    180 \u001b[0;31m        )\n",
            "\u001b[0m\u001b[0;32m    181 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(150)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    148 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    149 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 150 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    151 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    152 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mprior\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "tensor([[ 4.,  9.,  2.,  ..., 10.,  9.,  0.],\n",
            "        [ 7.,  6.,  6.,  ...,  7.,  1.,  4.],\n",
            "        [ 7.,  8.,  2.,  ...,  3.,  9.,  6.],\n",
            "        ...,\n",
            "        [ 5.,  4.,  6.,  ...,  3.,  1.,  8.],\n",
            "        [ 4.,  1.,  8.,  ...,  4.,  7.,  9.],\n",
            "        [ 8.,  2.,  6.,  ...,  8.,  6.,  6.]], dtype=torch.float32)\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(468)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    466 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    467 \u001b[0;31m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 468 \u001b[0;31m        \u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    469 \u001b[0;31m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    470 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "tensor([[ 4.,  9.,  2.,  ..., 10.,  9.,  0.],\n",
            "        [ 7.,  6.,  6.,  ...,  7.,  1.,  4.],\n",
            "        [ 7.,  8.,  2.,  ...,  3.,  9.,  6.],\n",
            "        ...,\n",
            "        [ 5.,  4.,  6.,  ...,  3.,  1.,  8.],\n",
            "        [ 4.,  1.,  8.,  ...,  4.,  7.,  9.],\n",
            "        [ 8.,  2.,  6.,  ...,  8.,  6.,  6.]], dtype=torch.float32)\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(583)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    581 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    582 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 583 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    584 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    585 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "tensor([[ 4.,  9.,  2.,  ..., 10.,  9.,  0.],\n",
            "        [ 7.,  6.,  6.,  ...,  7.,  1.,  4.],\n",
            "        [ 7.,  8.,  2.,  ...,  3.,  9.,  6.],\n",
            "        ...,\n",
            "        [ 5.,  4.,  6.,  ...,  3.,  1.,  8.],\n",
            "        [ 4.,  1.,  8.,  ...,  4.,  7.,  9.],\n",
            "        [ 8.,  2.,  6.,  ...,  8.,  6.,  6.]], dtype=torch.float32)\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m(469)\u001b[0;36m_train_batch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    467 \u001b[0;31m            \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    468 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 469 \u001b[0;31m        \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    470 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    471 \u001b[0;31m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "*** NameError: name 'x' is not defined\n",
            "tensor([[ 4.,  9.,  2.,  ..., 10.,  9.,  0.],\n",
            "        [ 7.,  6.,  6.,  ...,  7.,  1.,  4.],\n",
            "        [ 7.,  8.,  2.,  ...,  3.,  9.,  6.],\n",
            "        ...,\n",
            "        [ 5.,  4.,  6.,  ...,  3.,  1.,  8.],\n",
            "        [ 4.,  1.,  8.,  ...,  4.,  7.,  9.],\n",
            "        [ 8.,  2.,  6.,  ...,  8.,  6.,  6.]], dtype=torch.float32)\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m(434)\u001b[0;36m_train_epoch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    432 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    433 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 434 \u001b[0;31m            \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    435 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    436 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "tensor([[ 4.,  9.,  2.,  ..., 10.,  9.,  0.],\n",
            "        [ 7.,  6.,  6.,  ...,  7.,  1.,  4.],\n",
            "        [ 7.,  8.,  2.,  ...,  3.,  9.,  6.],\n",
            "        ...,\n",
            "        [ 5.,  4.,  6.,  ...,  3.,  1.,  8.],\n",
            "        [ 4.,  1.,  8.,  ...,  4.,  7.,  9.],\n",
            "        [ 8.,  2.,  6.,  ...,  8.,  6.,  6.]], dtype=torch.float32)\n",
            "--KeyboardInterrupt--\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m(223)\u001b[0;36mfit\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    221 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    222 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 223 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    224 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    225 \u001b[0;31m            \u001b[0;31m# Apply predict epoch to all eval sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TabNet_params = {\n",
        "    'n_d' : [2, 10],\n",
        "    'n_steps' : [1, 25],\n",
        "    'gamma' : [1., 2.],\n",
        "    'n_independent' : [1, 20],\n",
        "    'n_shared' : [1, 20],\n",
        "    'momentum' : [1e-3, 0.4],\n",
        "    'mask_type' : ['sparsemax', 'entmax'],\n",
        "    'nfold' : 5,\n",
        "    'squared_metrics' : False\n",
        "    }\n",
        "\n",
        "model_name = TabNet\n",
        "\n",
        "TabNet_res = main(X = X, y = y, model_name = model_name, params = TabNet_params, n_trials = 50)"
      ],
      "metadata": {
        "id": "MvBbX0jFu_o8",
        "outputId": "c3366fe0-86ca-4bba-df7d-842c89976d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6a250780adad47dea6eaf81dca2887b2",
            "192aa0879e27494b8a40d1ce74312a12",
            "b66be8e16e6d42948f42fbebec9ac70d",
            "b68b99b9b1b74dbdaf81b76d0547fab8",
            "af111a16c5694ef8b6589644ceeb58f1",
            "a53757e1f84f43569eda9cc2466fd618",
            "82a71f80e91e4b5eb8fec621d1088e5c",
            "33ac92d5194b466584905a119dcbfba9",
            "596ef9458afb4fc7a10e96644c988183",
            "e48b70c6c4c7420e96221c404dcdce74",
            "c03bd7e16c7345d7bead9cd33438b3cd"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-20 06:44:45,324]\u001b[0m A new study created in memory with name: no-name-76acffe7-550a-4925-ba9d-5fc664087939\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start hyperparameter optimization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/optuna/progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
            "  self._init_valid()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a250780adad47dea6eaf81dca2887b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_d': 3, 'n_steps': 8, 'gamma': 1.062036414714562, 'n_independent': 10, 'n_shared': 17, 'momentum': 0.2582866324854284, 'mask_type': 'entmax'}\n",
            "\n",
            "Early stopping occurred at epoch 173 with best_epoch = 153 and best_eval_mse = 2520.85343\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 248 with best_epoch = 228 and best_eval_mse = 2525.8012\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 181 with best_epoch = 161 and best_eval_mse = 2318.17918\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 191 with best_epoch = 171 and best_eval_mse = 2434.95432\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 180 with best_epoch = 160 and best_eval_mse = 2375.5705\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 06:56:23,327]\u001b[0m Trial 0 finished with value: 49.33960101210168 and parameters: {'n_d': 3, 'n_steps': 8, 'gamma': 1.062036414714562, 'n_independent': 10, 'n_shared': 17, 'momentum': 0.2582866324854284, 'mask_type': 'entmax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 4, 'n_steps': 17, 'gamma': 1.0933732568292283, 'n_independent': 2, 'n_shared': 12, 'momentum': 0.0078233485195872, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 148 with best_epoch = 128 and best_eval_mse = 3059.96241\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 71 with best_epoch = 51 and best_eval_mse = 14893.95326\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 195 with best_epoch = 175 and best_eval_mse = 2518.54997\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 47 and best_eval_mse = 8082.29093\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 218 with best_epoch = 198 and best_eval_mse = 2320.20042\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 07:05:37,315]\u001b[0m Trial 1 finished with value: 73.12257805940186 and parameters: {'n_d': 4, 'n_steps': 17, 'gamma': 1.0933732568292283, 'n_independent': 2, 'n_shared': 12, 'momentum': 0.0078233485195872, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 8, 'n_steps': 14, 'gamma': 1.2688600578882592, 'n_independent': 8, 'n_shared': 5, 'momentum': 0.0030558696912823184, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 50 and best_eval_mse = 5720.08405\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 52 and best_eval_mse = 4763.96152\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_mse = 5736.84629\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 76 with best_epoch = 56 and best_eval_mse = 4567.1396\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 61 and best_eval_mse = 4446.26703\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 07:09:38,568]\u001b[0m Trial 2 finished with value: 70.93112325425 and parameters: {'n_d': 8, 'n_steps': 14, 'gamma': 1.2688600578882592, 'n_independent': 8, 'n_shared': 5, 'momentum': 0.0030558696912823184, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 7, 'n_steps': 23, 'gamma': 1.6223388243205816, 'n_independent': 6, 'n_shared': 4, 'momentum': 0.13314761977292472, 'mask_type': 'entmax'}\n",
            "\n",
            "Early stopping occurred at epoch 116 with best_epoch = 96 and best_eval_mse = 2780.37896\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 87 with best_epoch = 67 and best_eval_mse = 2887.97749\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 68 and best_eval_mse = 2784.46464\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 68 and best_eval_mse = 3080.65314\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 144 with best_epoch = 124 and best_eval_mse = 2307.95252\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 07:16:43,747]\u001b[0m Trial 3 finished with value: 52.5564035497251 and parameters: {'n_d': 7, 'n_steps': 23, 'gamma': 1.6223388243205816, 'n_independent': 6, 'n_shared': 4, 'momentum': 0.13314761977292472, 'mask_type': 'entmax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 6, 'n_steps': 16, 'gamma': 1.5326204827750485, 'n_independent': 1, 'n_shared': 11, 'momentum': 0.21436099878075457, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_mse = 6019.1691\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 22 with best_epoch = 2 and best_eval_mse = 5654.16514\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_mse = 5970.03755\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_mse = 5666.34514\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 26 with best_epoch = 6 and best_eval_mse = 6100.21549\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 07:17:57,386]\u001b[0m Trial 4 finished with value: 76.6845022183434 and parameters: {'n_d': 6, 'n_steps': 16, 'gamma': 1.5326204827750485, 'n_independent': 1, 'n_shared': 11, 'momentum': 0.21436099878075457, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 4, 'n_steps': 16, 'gamma': 1.7261381221193408, 'n_independent': 10, 'n_shared': 16, 'momentum': 0.003152357319926508, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 204 with best_epoch = 184 and best_eval_mse = 9693.62572\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 156 with best_epoch = 136 and best_eval_mse = 10292.03232\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 136 with best_epoch = 116 and best_eval_mse = 10321.27407\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 177 with best_epoch = 157 and best_eval_mse = 8628.89033\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 199 with best_epoch = 179 and best_eval_mse = 11374.92515\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 07:37:06,950]\u001b[0m Trial 5 finished with value: 100.20893048180793 and parameters: {'n_d': 4, 'n_steps': 16, 'gamma': 1.7261381221193408, 'n_independent': 10, 'n_shared': 16, 'momentum': 0.003152357319926508, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 6, 'n_steps': 20, 'gamma': 1.115249678236137, 'n_independent': 14, 'n_shared': 8, 'momentum': 0.007864157288066484, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 247 with best_epoch = 227 and best_eval_mse = 9871.45275\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 262 with best_epoch = 242 and best_eval_mse = 10393.99264\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 171 with best_epoch = 151 and best_eval_mse = 10564.43817\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 198 with best_epoch = 178 and best_eval_mse = 8883.9274\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 198 with best_epoch = 178 and best_eval_mse = 11661.69883\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 08:02:04,271]\u001b[0m Trial 6 finished with value: 101.26670085477932 and parameters: {'n_d': 6, 'n_steps': 20, 'gamma': 1.115249678236137, 'n_independent': 14, 'n_shared': 8, 'momentum': 0.007864157288066484, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 5, 'n_steps': 24, 'gamma': 1.0912055708074124, 'n_independent': 7, 'n_shared': 11, 'momentum': 0.007138995115091403, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_eval_mse = 9130.97837\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_eval_mse = 9636.64942\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_eval_mse = 9497.319\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 83 with best_epoch = 63 and best_eval_mse = 8175.00063\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_eval_mse = 10584.91031\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 08:09:38,399]\u001b[0m Trial 7 finished with value: 96.89509658441692 and parameters: {'n_d': 5, 'n_steps': 24, 'gamma': 1.0912055708074124, 'n_independent': 7, 'n_shared': 11, 'momentum': 0.007138995115091403, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 2, 'n_steps': 18, 'gamma': 1.8359434095405658, 'n_independent': 9, 'n_shared': 17, 'momentum': 0.026471803687187, 'mask_type': 'entmax'}\n",
            "\n",
            "Early stopping occurred at epoch 110 with best_epoch = 90 and best_eval_mse = 9910.12376\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 114 with best_epoch = 94 and best_eval_mse = 10854.34192\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_eval_mse = 11185.47247\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[33m[W 2022-04-20 08:17:08,956]\u001b[0m Trial 8 failed because of the following error: RuntimeError('CUDA error: device-side assert triggered')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-3-fade56b529c6>\", line 25, in __call__\n",
            "    model.fit(X_train, y_train, X_val, y_val)\n",
            "  File \"<ipython-input-4-4b4cca892797>\", line 16, in fit\n",
            "    self.model.fit(X, y, eval_set = [(X_val, y_val)], eval_name = ['eval'], max_epochs = 500, patience = 20)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\", line 227, in fit\n",
            "    self._predict_epoch(eval_name, valid_dataloader)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\", line 504, in _predict_epoch\n",
            "    scores = self._predict_batch(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\", line 532, in _predict_batch\n",
            "    scores, _ = self.network(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\", line 583, in forward\n",
            "    return self.tabnet(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\", line 468, in forward\n",
            "    steps_output, M_loss = self.encoder(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\", line 160, in forward\n",
            "    M = self.att_transformers[step](prior, att)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\", line 637, in forward\n",
            "    x = self.selector(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\", line 204, in forward\n",
            "    return entmax15(input, self.dim)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\", line 127, in forward\n",
            "    tau_star, _ = Entmax15Function._threshold_and_support(input, dim)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\", line 159, in _threshold_and_support\n",
            "    tau_star = tau.gather(dim, support_size - 1)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-47bb779208d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mTabNet_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabNet_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-fade56b529c6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(X, y, model_name, params, n_trials)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mSampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-fade56b529c6>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             score += mean_squared_error(y_val, model.predict(X_val),\n\u001b[1;32m     27\u001b[0m                                         squared = self.params['squared_metrics'])\n",
            "\u001b[0;32m<ipython-input-4-4b4cca892797>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# Apply predict epoch to all eval sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# Call method on_epoch_end for all callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_predict_epoch\u001b[0;34m(self, name, loader)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# Main loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mlist_y_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0mlist_y_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_predict_batch\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# compute model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0msteps_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt_transformers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             M_loss += torch.mean(\n\u001b[1;32m    162\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, priors, processed_feat)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mentmax15\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, dim)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# divide by 2 to solve actual Entmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mtau_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEntmax15Function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threshold_and_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtau_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\u001b[0m in \u001b[0;36m_threshold_and_support\u001b[0;34m(input, dim)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0msupport_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mXsrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mtau_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtau_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%debug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5pMclaHZu4k",
        "outputId": "269ba03e-d5f8-4720-93aa-95f88c87eb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(738)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    736 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    737 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 738 \u001b[0;31m        \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    739 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# the first layer of the block has no scale multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    740 \u001b[0;31m            \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglu_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> x.device\n",
            "device(type='cuda', index=0)\n",
            "ipdb> self.glu_layers\n",
            "ModuleList(\n",
            "  (0): GLU_Layer(\n",
            "    (fc): Linear(in_features=50, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (2): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (3): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (4): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (5): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (6): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (7): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (8): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (9): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (10): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (11): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (12): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (13): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (14): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (15): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (16): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(703)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    701 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    702 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 703 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    704 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecifics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    705 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> self.shared\n",
            "GLU_Block(\n",
            "  (shared_layers): ModuleList(\n",
            "    (0): Linear(in_features=50, out_features=20, bias=False)\n",
            "    (1): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (2): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (3): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (4): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (5): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (6): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (7): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (8): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (9): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (10): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (11): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (12): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (13): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (14): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (15): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (16): Linear(in_features=10, out_features=20, bias=False)\n",
            "  )\n",
            "  (glu_layers): ModuleList(\n",
            "    (0): GLU_Layer(\n",
            "      (fc): Linear(in_features=50, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (2): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (3): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (4): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (9): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (10): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (11): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (12): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (13): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (14): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (15): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (16): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "ipdb> self.specifics\n",
            "GLU_Block(\n",
            "  (glu_layers): ModuleList(\n",
            "    (0): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (2): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (3): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (4): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> d\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(703)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    701 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    702 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 703 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    704 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecifics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    705 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> г\n",
            "*** NameError: name 'г' is not defined\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(168)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    166 \u001b[0;31m            \u001b[0;31m# output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    167 \u001b[0;31m            \u001b[0mmasked_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 168 \u001b[0;31m            \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_transformers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    169 \u001b[0;31m            \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    170 \u001b[0;31m            \u001b[0msteps_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> print(step)\n",
            "0\n",
            "ipdb> masked_x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> print(out)\n",
            "*** NameError: name 'out' is not defined\n",
            "ipdb> M\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(468)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    466 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    467 \u001b[0;31m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 468 \u001b[0;31m        \u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    469 \u001b[0;31m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    470 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> M_loss\n",
            "*** NameError: name 'M_loss' is not defined\n",
            "ipdb> steps_output\n",
            "*** NameError: name 'steps_output' is not defined\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(583)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    581 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    582 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 583 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    584 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    585 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m(532)\u001b[0;36m_predict_batch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    530 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    531 \u001b[0;31m        \u001b[0;31m# compute model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 532 \u001b[0;31m        \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    533 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    534 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> X\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> scores\n",
            "*** NameError: name 'scores' is not defined\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m(504)\u001b[0;36m_predict_epoch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    502 \u001b[0;31m        \u001b[0;31m# Main loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    503 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 504 \u001b[0;31m            \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    505 \u001b[0;31m            \u001b[0mlist_y_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    506 \u001b[0;31m            \u001b[0mlist_y_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> X\n",
            "tensor([[ 9.,  7.,  1.,  ...,  9.,  1.,  3.],\n",
            "        [ 0.,  5.,  0.,  ...,  2.,  1., 10.],\n",
            "        [ 7., 10.,  7.,  ...,  3.,  7.,  0.],\n",
            "        ...,\n",
            "        [ 2., 10.,  7.,  ...,  8.,  2.,  7.],\n",
            "        [ 8.,  5.,  5.,  ...,  7.,  9.,  4.],\n",
            "        [ 7.,  4.,  9.,  ...,  8., 10., 10.]])\n",
            "--KeyboardInterrupt--\n",
            "ipdb> q\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 357, in set_quit\n",
            "    sys.settrace(None)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.astype(np.float64)"
      ],
      "metadata": {
        "id": "Lczj4jHk1Sec",
        "outputId": "52c695ac-e0a2-4609-c1ed-d80a06c4d6bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8.,  8.,  0., ...,  2.,  2.,  2.],\n",
              "       [ 0.,  9.,  7., ...,  7.,  0.,  3.],\n",
              "       [ 2.,  3.,  4., ...,  9.,  9.,  4.],\n",
              "       ...,\n",
              "       [ 7.,  2.,  8., ...,  4.,  9.,  8.],\n",
              "       [ 8.,  0.,  1., ...,  3., 10.,  6.],\n",
              "       [ 5.,  0.,  0., ...,  5.,  6.,  5.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /usr/local && pwd"
      ],
      "metadata": {
        "id": "e8rFKRMH1Sxw",
        "outputId": "1d4ac426-96f3-4898-c051-9d1f54bec116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N3bDJv6HM_8i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}