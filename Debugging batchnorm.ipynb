{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Creating classes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E5MrtNW8V1KJ"
      ],
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a250780adad47dea6eaf81dca2887b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_192aa0879e27494b8a40d1ce74312a12",
              "IPY_MODEL_b66be8e16e6d42948f42fbebec9ac70d",
              "IPY_MODEL_b68b99b9b1b74dbdaf81b76d0547fab8"
            ],
            "layout": "IPY_MODEL_af111a16c5694ef8b6589644ceeb58f1"
          }
        },
        "192aa0879e27494b8a40d1ce74312a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a53757e1f84f43569eda9cc2466fd618",
            "placeholder": "​",
            "style": "IPY_MODEL_82a71f80e91e4b5eb8fec621d1088e5c",
            "value": " 16%"
          }
        },
        "b66be8e16e6d42948f42fbebec9ac70d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ac92d5194b466584905a119dcbfba9",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_596ef9458afb4fc7a10e96644c988183",
            "value": 8
          }
        },
        "b68b99b9b1b74dbdaf81b76d0547fab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48b70c6c4c7420e96221c404dcdce74",
            "placeholder": "​",
            "style": "IPY_MODEL_c03bd7e16c7345d7bead9cd33438b3cd",
            "value": " 8/50 [1:32:23&lt;8:45:02, 750.07s/it]"
          }
        },
        "af111a16c5694ef8b6589644ceeb58f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a53757e1f84f43569eda9cc2466fd618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82a71f80e91e4b5eb8fec621d1088e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33ac92d5194b466584905a119dcbfba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "596ef9458afb4fc7a10e96644c988183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e48b70c6c4c7420e96221c404dcdce74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c03bd7e16c7345d7bead9cd33438b3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "!pip install pytorch-tabnet\n",
        "!pip install optuna\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import optuna\n",
        "from google.colab import output\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from pytorch_tabnet.tab_model import  TabNetRegressor\n",
        "\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "u0T8S_xHsBRP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Objective(object):\n",
        "    def __init__(self, model_name, X, y, params):\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Save the trainings data\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.params = params\n",
        "\n",
        "        \n",
        "    def __call__(self, trial):\n",
        "        # Define hyperparameters to optimize\n",
        "        trial_params = self.model_name.define_trial_parameters(trial, self.params)\n",
        "        print(trial_params)\n",
        "        \n",
        "        score = 0\n",
        "        # Cross validate the chosen hyperparameters\n",
        "\n",
        "        kf = KFold(self.params['nfold'], shuffle = False)\n",
        "        for train, test in kf.split(self.X):\n",
        "            X_train, y_train = self.X.iloc[train, :], self.y.iloc[train]\n",
        "            X_val, y_val = self.X.iloc[test, :], self.y.iloc[test]\n",
        "            \n",
        "            model = self.model_name(trial_params)\n",
        "            model.fit(X_train, y_train, X_val, y_val)\n",
        "            score += mean_squared_error(y_val, model.predict(X_val),\n",
        "                                        squared = self.params['squared_metrics'])\n",
        "\n",
        "        score /= self.params['nfold']\n",
        "        \n",
        "        return score\n",
        "\n",
        "\n",
        "def main(X, y, model_name, params, n_trials = 100):\n",
        "    print(\"Start hyperparameter optimization\")\n",
        "    \n",
        "    Sampler = optuna.samplers.TPESampler(seed = 777)\n",
        "    study = optuna.create_study(sampler = Sampler)\n",
        "    study.optimize(Objective(model_name, X, y, params), n_trials, show_progress_bar = True, n_jobs = 1)\n",
        "    \n",
        "    print(\"Best parameters:\", study.best_trial.params)\n",
        "\n",
        "    return study"
      ],
      "metadata": {
        "id": "MuGwONKNukQ9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TabNet():\n",
        "\n",
        "    def __init__(self, params):\n",
        "        \n",
        "        self.model = TabNetRegressor(**params, verbose = False, device_name = 'cuda')\n",
        "        #if torch.cuda.is_available():\n",
        "        #    self.model.to('cuda')\n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        X = X.to_numpy()\n",
        "        y = y.to_numpy().reshape(-1, 1)\n",
        "        \n",
        "        if isinstance(X_val, pd.DataFrame):\n",
        "            X_val, y_val = X_val.to_numpy(), y_val.to_numpy().reshape(-1, 1)\n",
        "            \n",
        "        self.model.fit(X, y, eval_set = [(X_val, y_val)], eval_name = ['eval'], max_epochs = 500, patience = 20)\n",
        "        history = self.model.history\n",
        "        return history['loss']\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = X.to_numpy()\n",
        "\n",
        "        return self.model.predict(X)\n",
        "        \n",
        "    @classmethod\n",
        "    def define_trial_parameters(cls, trial, params):\n",
        "        params_tunable = {}\n",
        "        params_out = {}\n",
        "        for i, val in params.items():\n",
        "            if isinstance(val, list):\n",
        "                params_tunable[f'{i}'] = val\n",
        "            else:\n",
        "                params_out[f'{i}'] = val\n",
        "        \n",
        "        if 'n_d' in params_tunable:\n",
        "            params_out[f'n_d'] = trial.suggest_int('n_d', params['n_d'][0], params['n_d'][1], log = False)\n",
        "        if 'n_steps' in params_tunable:\n",
        "            params_out[f'n_steps'] = trial.suggest_int('n_steps', params['n_steps'][0], params['n_steps'][1], log = False)\n",
        "        if 'gamma' in params_tunable:\n",
        "            params_out[f'gamma'] = trial.suggest_float('gamma', params['gamma'][0], params['gamma'][1], log = False)\n",
        "        if 'cat_emb_dim' in params_tunable:\n",
        "            params_out[f'cat_emb_dim'] = trial.suggest_int('cat_emb_dim', params['cat_emb_dim'][0], params['cat_emb_dim'][1], log = False)\n",
        "        if 'n_independent' in params_tunable:\n",
        "            params_out[f'n_independent'] = trial.suggest_int('n_independent', params['n_independent'][0], params['n_independent'][1], log = False)\n",
        "        if 'n_shared' in params_tunable:\n",
        "            params_out[f'n_shared'] = trial.suggest_int('n_shared', params['n_shared'][0], params['n_shared'][1], log = False)\n",
        "        if 'momentum' in params_tunable:\n",
        "            params_out[f'momentum'] = trial.suggest_float('momentum', params['momentum'][0], params['momentum'][1], log = True)\n",
        "        if 'mask_type' in params_tunable:\n",
        "            params_out[f'mask_type'] = trial.suggest_categorical('mask_type', params['mask_type'])\n",
        "        \n",
        "        \n",
        "        if 'nfold' in params_out:\n",
        "            del params_out['nfold']\n",
        "        if 'squared_metrics' in params_out:\n",
        "            del params_out['squared_metrics']\n",
        "        \n",
        "        return params_out"
      ],
      "metadata": {
        "id": "GKqdTyCVtzwO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(7)\n",
        "X = np.random.randint(0, 11, size = (745, 50))\n",
        "y = np.random.rand(745) * 175"
      ],
      "metadata": {
        "id": "Nwpw5oRFsGup"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(X)\n",
        "y = pd.DataFrame(y)\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "AZml6V81u4M_",
        "outputId": "f4a1e660-f891-463e-c8a0-edf447f6cc70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TabNet_params = {\n",
        "    'n_d' : [2, 10],\n",
        "    'n_steps' : [1, 25],\n",
        "    'gamma' : [1., 2.],\n",
        "    'n_independent' : [1, 20],\n",
        "    'n_shared' : [1, 20],\n",
        "    'momentum' : [1e-3, 0.4],\n",
        "    'mask_type' : ['sparsemax', 'entmax'],\n",
        "    'nfold' : 5,\n",
        "    'squared_metrics' : False\n",
        "    }\n",
        "\n",
        "model_name = TabNet\n",
        "\n",
        "TabNet_res = main(X = X, y = y, model_name = model_name, params = TabNet_params, n_trials = 50)"
      ],
      "metadata": {
        "id": "MvBbX0jFu_o8",
        "outputId": "c3366fe0-86ca-4bba-df7d-842c89976d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6a250780adad47dea6eaf81dca2887b2",
            "192aa0879e27494b8a40d1ce74312a12",
            "b66be8e16e6d42948f42fbebec9ac70d",
            "b68b99b9b1b74dbdaf81b76d0547fab8",
            "af111a16c5694ef8b6589644ceeb58f1",
            "a53757e1f84f43569eda9cc2466fd618",
            "82a71f80e91e4b5eb8fec621d1088e5c",
            "33ac92d5194b466584905a119dcbfba9",
            "596ef9458afb4fc7a10e96644c988183",
            "e48b70c6c4c7420e96221c404dcdce74",
            "c03bd7e16c7345d7bead9cd33438b3cd"
          ]
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-20 06:44:45,324]\u001b[0m A new study created in memory with name: no-name-76acffe7-550a-4925-ba9d-5fc664087939\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start hyperparameter optimization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/optuna/progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
            "  self._init_valid()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a250780adad47dea6eaf81dca2887b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_d': 3, 'n_steps': 8, 'gamma': 1.062036414714562, 'n_independent': 10, 'n_shared': 17, 'momentum': 0.2582866324854284, 'mask_type': 'entmax'}\n",
            "\n",
            "Early stopping occurred at epoch 173 with best_epoch = 153 and best_eval_mse = 2520.85343\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 248 with best_epoch = 228 and best_eval_mse = 2525.8012\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 181 with best_epoch = 161 and best_eval_mse = 2318.17918\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 191 with best_epoch = 171 and best_eval_mse = 2434.95432\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 180 with best_epoch = 160 and best_eval_mse = 2375.5705\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 06:56:23,327]\u001b[0m Trial 0 finished with value: 49.33960101210168 and parameters: {'n_d': 3, 'n_steps': 8, 'gamma': 1.062036414714562, 'n_independent': 10, 'n_shared': 17, 'momentum': 0.2582866324854284, 'mask_type': 'entmax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 4, 'n_steps': 17, 'gamma': 1.0933732568292283, 'n_independent': 2, 'n_shared': 12, 'momentum': 0.0078233485195872, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 148 with best_epoch = 128 and best_eval_mse = 3059.96241\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 71 with best_epoch = 51 and best_eval_mse = 14893.95326\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 195 with best_epoch = 175 and best_eval_mse = 2518.54997\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 47 and best_eval_mse = 8082.29093\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 218 with best_epoch = 198 and best_eval_mse = 2320.20042\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 07:05:37,315]\u001b[0m Trial 1 finished with value: 73.12257805940186 and parameters: {'n_d': 4, 'n_steps': 17, 'gamma': 1.0933732568292283, 'n_independent': 2, 'n_shared': 12, 'momentum': 0.0078233485195872, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 8, 'n_steps': 14, 'gamma': 1.2688600578882592, 'n_independent': 8, 'n_shared': 5, 'momentum': 0.0030558696912823184, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 50 and best_eval_mse = 5720.08405\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 52 and best_eval_mse = 4763.96152\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_mse = 5736.84629\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 76 with best_epoch = 56 and best_eval_mse = 4567.1396\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 61 and best_eval_mse = 4446.26703\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 07:09:38,568]\u001b[0m Trial 2 finished with value: 70.93112325425 and parameters: {'n_d': 8, 'n_steps': 14, 'gamma': 1.2688600578882592, 'n_independent': 8, 'n_shared': 5, 'momentum': 0.0030558696912823184, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 7, 'n_steps': 23, 'gamma': 1.6223388243205816, 'n_independent': 6, 'n_shared': 4, 'momentum': 0.13314761977292472, 'mask_type': 'entmax'}\n",
            "\n",
            "Early stopping occurred at epoch 116 with best_epoch = 96 and best_eval_mse = 2780.37896\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 87 with best_epoch = 67 and best_eval_mse = 2887.97749\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 68 and best_eval_mse = 2784.46464\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 68 and best_eval_mse = 3080.65314\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 144 with best_epoch = 124 and best_eval_mse = 2307.95252\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 07:16:43,747]\u001b[0m Trial 3 finished with value: 52.5564035497251 and parameters: {'n_d': 7, 'n_steps': 23, 'gamma': 1.6223388243205816, 'n_independent': 6, 'n_shared': 4, 'momentum': 0.13314761977292472, 'mask_type': 'entmax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 6, 'n_steps': 16, 'gamma': 1.5326204827750485, 'n_independent': 1, 'n_shared': 11, 'momentum': 0.21436099878075457, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_mse = 6019.1691\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 22 with best_epoch = 2 and best_eval_mse = 5654.16514\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_mse = 5970.03755\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_mse = 5666.34514\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 26 with best_epoch = 6 and best_eval_mse = 6100.21549\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 07:17:57,386]\u001b[0m Trial 4 finished with value: 76.6845022183434 and parameters: {'n_d': 6, 'n_steps': 16, 'gamma': 1.5326204827750485, 'n_independent': 1, 'n_shared': 11, 'momentum': 0.21436099878075457, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 4, 'n_steps': 16, 'gamma': 1.7261381221193408, 'n_independent': 10, 'n_shared': 16, 'momentum': 0.003152357319926508, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 204 with best_epoch = 184 and best_eval_mse = 9693.62572\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 156 with best_epoch = 136 and best_eval_mse = 10292.03232\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 136 with best_epoch = 116 and best_eval_mse = 10321.27407\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 177 with best_epoch = 157 and best_eval_mse = 8628.89033\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 199 with best_epoch = 179 and best_eval_mse = 11374.92515\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 07:37:06,950]\u001b[0m Trial 5 finished with value: 100.20893048180793 and parameters: {'n_d': 4, 'n_steps': 16, 'gamma': 1.7261381221193408, 'n_independent': 10, 'n_shared': 16, 'momentum': 0.003152357319926508, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 6, 'n_steps': 20, 'gamma': 1.115249678236137, 'n_independent': 14, 'n_shared': 8, 'momentum': 0.007864157288066484, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 247 with best_epoch = 227 and best_eval_mse = 9871.45275\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 262 with best_epoch = 242 and best_eval_mse = 10393.99264\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 171 with best_epoch = 151 and best_eval_mse = 10564.43817\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 198 with best_epoch = 178 and best_eval_mse = 8883.9274\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 198 with best_epoch = 178 and best_eval_mse = 11661.69883\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 08:02:04,271]\u001b[0m Trial 6 finished with value: 101.26670085477932 and parameters: {'n_d': 6, 'n_steps': 20, 'gamma': 1.115249678236137, 'n_independent': 14, 'n_shared': 8, 'momentum': 0.007864157288066484, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 5, 'n_steps': 24, 'gamma': 1.0912055708074124, 'n_independent': 7, 'n_shared': 11, 'momentum': 0.007138995115091403, 'mask_type': 'sparsemax'}\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_eval_mse = 9130.97837\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_eval_mse = 9636.64942\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_eval_mse = 9497.319\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 83 with best_epoch = 63 and best_eval_mse = 8175.00063\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_eval_mse = 10584.91031\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[32m[I 2022-04-20 08:09:38,399]\u001b[0m Trial 7 finished with value: 96.89509658441692 and parameters: {'n_d': 5, 'n_steps': 24, 'gamma': 1.0912055708074124, 'n_independent': 7, 'n_shared': 11, 'momentum': 0.007138995115091403, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 49.33960101210168.\u001b[0m\n",
            "{'n_d': 2, 'n_steps': 18, 'gamma': 1.8359434095405658, 'n_independent': 9, 'n_shared': 17, 'momentum': 0.026471803687187, 'mask_type': 'entmax'}\n",
            "\n",
            "Early stopping occurred at epoch 110 with best_epoch = 90 and best_eval_mse = 9910.12376\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 114 with best_epoch = 94 and best_eval_mse = 10854.34192\n",
            "Best weights from best epoch are automatically used!\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_eval_mse = 11185.47247\n",
            "Best weights from best epoch are automatically used!\n",
            "\u001b[33m[W 2022-04-20 08:17:08,956]\u001b[0m Trial 8 failed because of the following error: RuntimeError('CUDA error: device-side assert triggered')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-3-fade56b529c6>\", line 25, in __call__\n",
            "    model.fit(X_train, y_train, X_val, y_val)\n",
            "  File \"<ipython-input-4-4b4cca892797>\", line 16, in fit\n",
            "    self.model.fit(X, y, eval_set = [(X_val, y_val)], eval_name = ['eval'], max_epochs = 500, patience = 20)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\", line 227, in fit\n",
            "    self._predict_epoch(eval_name, valid_dataloader)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\", line 504, in _predict_epoch\n",
            "    scores = self._predict_batch(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\", line 532, in _predict_batch\n",
            "    scores, _ = self.network(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\", line 583, in forward\n",
            "    return self.tabnet(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\", line 468, in forward\n",
            "    steps_output, M_loss = self.encoder(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\", line 160, in forward\n",
            "    M = self.att_transformers[step](prior, att)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\", line 637, in forward\n",
            "    x = self.selector(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\", line 204, in forward\n",
            "    return entmax15(input, self.dim)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\", line 127, in forward\n",
            "    tau_star, _ = Entmax15Function._threshold_and_support(input, dim)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\", line 159, in _threshold_and_support\n",
            "    tau_star = tau.gather(dim, support_size - 1)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-47bb779208d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mTabNet_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabNet_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-fade56b529c6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(X, y, model_name, params, n_trials)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mSampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-fade56b529c6>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             score += mean_squared_error(y_val, model.predict(X_val),\n\u001b[1;32m     27\u001b[0m                                         squared = self.params['squared_metrics'])\n",
            "\u001b[0;32m<ipython-input-4-4b4cca892797>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# Apply predict epoch to all eval sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# Call method on_epoch_end for all callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_predict_epoch\u001b[0;34m(self, name, loader)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# Main loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mlist_y_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0mlist_y_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_predict_batch\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# compute model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0msteps_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt_transformers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             M_loss += torch.mean(\n\u001b[1;32m    162\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, priors, processed_feat)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mentmax15\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, dim)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# divide by 2 to solve actual Entmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mtau_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEntmax15Function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threshold_and_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtau_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/sparsemax.py\u001b[0m in \u001b[0;36m_threshold_and_support\u001b[0;34m(input, dim)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0msupport_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mXsrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mtau_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtau_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%debug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5pMclaHZu4k",
        "outputId": "269ba03e-d5f8-4720-93aa-95f88c87eb1e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(738)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    736 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    737 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 738 \u001b[0;31m        \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    739 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# the first layer of the block has no scale multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    740 \u001b[0;31m            \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglu_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> x.device\n",
            "device(type='cuda', index=0)\n",
            "ipdb> self.glu_layers\n",
            "ModuleList(\n",
            "  (0): GLU_Layer(\n",
            "    (fc): Linear(in_features=50, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (2): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (3): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (4): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (5): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (6): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (7): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (8): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (9): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (10): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (11): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (12): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (13): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (14): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (15): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (16): GLU_Layer(\n",
            "    (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (bn): GBN(\n",
            "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(703)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    701 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    702 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 703 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    704 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecifics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    705 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> self.shared\n",
            "GLU_Block(\n",
            "  (shared_layers): ModuleList(\n",
            "    (0): Linear(in_features=50, out_features=20, bias=False)\n",
            "    (1): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (2): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (3): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (4): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (5): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (6): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (7): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (8): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (9): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (10): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (11): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (12): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (13): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (14): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (15): Linear(in_features=10, out_features=20, bias=False)\n",
            "    (16): Linear(in_features=10, out_features=20, bias=False)\n",
            "  )\n",
            "  (glu_layers): ModuleList(\n",
            "    (0): GLU_Layer(\n",
            "      (fc): Linear(in_features=50, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (2): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (3): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (4): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (9): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (10): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (11): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (12): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (13): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (14): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (15): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (16): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "ipdb> self.specifics\n",
            "GLU_Block(\n",
            "  (glu_layers): ModuleList(\n",
            "    (0): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (2): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (3): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (4): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): GLU_Layer(\n",
            "      (fc): Linear(in_features=10, out_features=20, bias=False)\n",
            "      (bn): GBN(\n",
            "        (bn): BatchNorm1d(20, eps=1e-05, momentum=0.026471803687187, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> d\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(703)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    701 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    702 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 703 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    704 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecifics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    705 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> г\n",
            "*** NameError: name 'г' is not defined\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(168)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    166 \u001b[0;31m            \u001b[0;31m# output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    167 \u001b[0;31m            \u001b[0mmasked_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 168 \u001b[0;31m            \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_transformers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    169 \u001b[0;31m            \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    170 \u001b[0;31m            \u001b[0msteps_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> print(step)\n",
            "0\n",
            "ipdb> masked_x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> print(out)\n",
            "*** NameError: name 'out' is not defined\n",
            "ipdb> M\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(468)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    466 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    467 \u001b[0;31m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 468 \u001b[0;31m        \u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    469 \u001b[0;31m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    470 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> M_loss\n",
            "*** NameError: name 'M_loss' is not defined\n",
            "ipdb> steps_output\n",
            "*** NameError: name 'steps_output' is not defined\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m(583)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    581 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    582 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 583 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    584 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    585 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> x\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
            "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
            "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m(532)\u001b[0;36m_predict_batch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    530 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    531 \u001b[0;31m        \u001b[0;31m# compute model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 532 \u001b[0;31m        \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    533 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    534 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> X\n",
            "*** RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "ipdb> scores\n",
            "*** NameError: name 'scores' is not defined\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m(504)\u001b[0;36m_predict_epoch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    502 \u001b[0;31m        \u001b[0;31m# Main loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    503 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 504 \u001b[0;31m            \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    505 \u001b[0;31m            \u001b[0mlist_y_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    506 \u001b[0;31m            \u001b[0mlist_y_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> X\n",
            "tensor([[ 9.,  7.,  1.,  ...,  9.,  1.,  3.],\n",
            "        [ 0.,  5.,  0.,  ...,  2.,  1., 10.],\n",
            "        [ 7., 10.,  7.,  ...,  3.,  7.,  0.],\n",
            "        ...,\n",
            "        [ 2., 10.,  7.,  ...,  8.,  2.,  7.],\n",
            "        [ 8.,  5.,  5.,  ...,  7.,  9.,  4.],\n",
            "        [ 7.,  4.,  9.,  ...,  8., 10., 10.]])\n",
            "--KeyboardInterrupt--\n",
            "ipdb> q\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 357, in set_quit\n",
            "    sys.settrace(None)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}